<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Web Test Automation - Day 7</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
            line-height: 1.6;
            color: #333;
        }
        h1, h2, h3 {
            color: #0056b3;
        }
        pre {
            background-color: #f4f4f4;
            padding: 10px;
            border-radius: 5px;
            overflow-x: auto;
        }
        code {
            font-family: "Courier New", Courier, monospace;
            color: #d44950;
        }
        .slide {
            margin-bottom: 40px;
            padding: 20px;
            border: 1px solid #ddd;
            border-radius: 8px;
            background-color: #fff;
        }
        .tree {
        background: #fff;
        padding: 16px;
        border-radius: 8px;
        box-shadow: 0 6px 18px rgba(16,24,40,0.06);
        max-width: 640px;
        font-family: "Courier New", monospace;
        white-space: pre;
        }
    </style>
    <link rel="stylesheet" 
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
</head>
<body>
    <h1>Web Test Automation - Day 7</h1>
    <h3>Running tests in diffent browsers and in parallel</h3>

    <div class="slide">
        <h2>Agenda</h2>
        <ul>
            <li>Updating conftest to run same test case in different browsers</li>
            <li>Generating new report with execution data</li>
            <li>What are hooks and some examples</li>
            <li>Pytest fixture scopes</li>
            <li>Parallel Execution with `pytest-xdist`</li>
        </ul>
    </div>
    <div class="slide">
        <h2>1. Updating conftest.py </h2>
        <p>Les's update our conftest.py to execute same test case in different browsers</p>
        <h3>On <code>conftest.py</code>:</h3>
        <pre><code class="language-python">import pytest
from selenium import webdriver
import json, time
from pathlib import Path
import os, pytest_html

# def pytest_addoption(parser):
#     parser.addoption("--browser", action="store", default="chrome", help="browser to execute tests (chrome or firefox)")

# @pytest.fixture
@pytest.fixture(params=["chrome", "firefox"], scope="function")
def driver(request):
    # browser = request.config.getoption("--browser").lower()
    browser = request.param
    if browser == "chrome":
        driver_instance = webdriver.Chrome()
    elif browser == "firefox":
        driver_instance = webdriver.Firefox()
    else:
        raise ValueError(f"Browser '{browser}' is not supported.")
    
    driver_instance.maximize_window()
    request.node.browser = browser

    yield driver_instance
    driver_instance.quit()</code></pre>

        <h4>Running a example:</h4>
        <p>Check your test case will run on Firefox and Chromes</p>
        <pre><code>pytest tests/test_tool_tips.py -v </code></pre>
        <h4>Output:</h4>
        <pre><code class="language-python">rootdir: /Users/rodrigoprado/rgp/ProgramaEstagio/estagio-automacao
configfile: pytest.ini
plugins: allure-pytest-2.15.0, html-4.1.1, xdist-3.8.0, metadata-3.1.1, cov-6.3.0
collected 4 items                                                                                                                                                               

tests/test_tool_tips_original.py::test_button_tooltip[chrome] PASSED                                                                                                      [ 25%]
tests/test_tool_tips_original.py::test_button_tooltip[firefox] PASSED                                                                                                     [ 50%]
tests/test_tool_tips_original.py::test_field_tooltip[chrome] PASSED                                                                                                       [ 75%]
tests/test_tool_tips_original.py::test_field_tooltip[firefox] PASSED   </code></pre>
    </div>
     <div class="slide">
        <h2>2. Generating new report with execution data`</h2>
        <p>Let's update our <strong><code>conftest.py</code></strong> again to create a new report with the execution data</p>
        
        <pre><code class="language-python">import csv

report_data = []

@pytest.hookimpl(hookwrapper=True)
def pytest_runtest_makereport(item, call):

    outcome = yield
    report = outcome.get_result()
    extra = getattr(report, "extra", [])
    if report.when == "call": # and report.failed:

        status = 'Passed' if report.passed else 'Failed'
        browser = getattr(item, 'browser', 'N/A')
        test_name = item.name
        duration = f"{report.duration:.4f}s"

        if report.failed:
            # Create screenshots directory if it doesn't exist
            if not os.path.exists("screenshots"):
                os.makedirs("screenshots")
            # Take screenshot
            driver = item.funcargs['driver']
            screenshot_file = os.path.join("screenshots", f"{item.name}_error.png")
            driver.save_screenshot(screenshot_file)
            # Add screenshot to the HTML report
            if screenshot_file:
                html = f'&lt;div&gt;&lt;img src="{screenshot_file}" alt="screenshot" style="width:304px;height:228px;" ' \
            f'onclick="window.open(this.src)" align="right"/&gt;&lt;/div&gt;'
                    extra.append(pytest_html.extras.html(html))
        
        report_data.append({
            "browser": browser.capitalize(),
            "test_case_name": test_name,
            "status": status,
            "timestamp": duration
        })

    report.extra = extra

def pytest_sessionfinish(session):
    """
    Hook executed in the end of test session to create the CSV report.
    """
    if not report_data:
        return
        
    # Ordena os dados por navegador para um relatório mais legível
    sorted_reports = sorted(report_data, key=lambda x: x['navegador'])
    
    keys = sorted_reports[0].keys()
    
    with open('test_report.csv', 'w', newline='') as output_file:
        dict_writer = csv.DictWriter(output_file, fieldnames=keys)
        dict_writer.writeheader()
        dict_writer.writerows(sorted_reports)

    print("\nReport 'test_report.csv' generated successfully")</code></pre>
        <h3>Run any test again, for instance:</h3>
        <pre><code class="language-python">pytest tests/test_tool_tips_original.py -v</code></pre>
        <h4>Output:</h4>
        <pre><code class="language-python">rootdir: /Users/rodrigoprado/rgp/ProgramaEstagio/estagio-automacao
configfile: pytest.ini
plugins: allure-pytest-2.15.0, html-4.1.1, xdist-3.8.0, metadata-3.1.1, cov-6.3.0
collected 4 items                                                                                                                                                               

tests/test_tool_tips_original.py::test_button_tooltip[chrome] PASSED                                                                                                      [ 25%]
tests/test_tool_tips_original.py::test_button_tooltip[firefox] PASSED                                                                                                     [ 50%]
tests/test_tool_tips_original.py::test_field_tooltip[chrome] PASSED                                                                                                       [ 75%]
tests/test_tool_tips_original.py::test_field_tooltip[firefox] PASSED   </code></pre>
<h4>Check 'test_report.csv file created in the project root</h4>
<p><img src="screenshots/test_report.png" alt="HTML report created" style="max-width:auto; height:150px;"></p>
<pre><code># test_report.csv
browser,test_case_name,status,timestamp
Chrome,test_button_tooltip[chrome],Passed,5.1125s
Chrome,test_field_tooltip[chrome],Failed,14.3820s
Firefox,test_button_tooltip[firefox],Passed,8.5066s
Firefox,test_field_tooltip[firefox],Passed,8.2310s</code></pre>
    </div>
    <div class="slide">
        <h2>3. Pytest Hooks</h2>
        <p><strong>What are Pytest Hooks?</strong></p>
        <ul>
            <li>Pytest hooks are "plug-in" points that let you extend or customize how pytest behaves before, during, or after a test run.</li>
            <li>They are functions with specific names that pytest automatically looks for and calls at different stages of the test process.</li>
            <li>Think of them like event listeners or interceptors in your testing lifecycle.</li>
        </ul>
        <p><strong>Why Use Hooks?</strong></p>
        <p>Let’s say you want to:</p>
        <ul>
            <li>Add custom logging before each test.</li>
            <li>Connect to a test database before the session starts.</li>
            <li>Modify test items dynamically.</li>
            <li>Collect extra metadata during the test run.</li>
        </ul>
        <p>Pytest hooks let you do all this <strong> without touching your actual tests</strong></p>
        <h3>Session-Level Hooks</h3>
<pre><code class="language-python"># conftest.py

def pytest_sessionstart(session):
    # Called before tests run
    pass

def pytest_sessionfinish(session, exitstatus):
    # Called after all tests run
    pass</code></pre>
    <h3>Test Collection Hooks</h3>
    <pre><code>def pytest_collection_modifyitems(session, config, items):
    items.reverse()  # run tests in reverse order</code></pre>

<pre><code class="language-python">def pytest_collection_modifyitems(config, items):
    for item in items:
        if "slow" in item.keywords and not config.getoption("--runslow"):
            item.add_marker(pytest.mark.skip(reason="need --runslow option to run"))</code></pre>

            <h3>Test Execution Hooks</h3>
    <pre><code>def pytest_runtest_setup(item):
    # Before test

def pytest_runtest_call(item):
    # Actual test call

def pytest_runtest_teardown(item):
    # After test</code></pre>
    <h3>Reporting Hooks</h3>
    <pre><code>@pytest.hookimpl(hookwrapper=True)
def pytest_runtest_makereport(item, call):

    outcome = yield
    report = outcome.get_result()
    extra = getattr(report, "extra", [])
    if report.when == "call": # and report.failed:

        status = 'Passed' if report.passed else 'Failed'
        browser = getattr(item, 'browser', 'N/A')
        test_name = item.name
        duration = f"{report.duration:.4f}s"</code></pre>

    </div>
    <div class="slide">
        <h2>5. What Are Pytest Fixture Scopes?</h2>
        <p>Pytest allows you to define fixtures with different scopes, which determine how often the fixture is invoked.</p>
        <table border="1" cellpadding="8" cellspacing="0">
  <thead>
    <tr>
      <th>Scope</th>
      <th>How Often It Runs</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>function</code></td>
      <td>Every test function (default)</td>
    </tr>
    <tr>
      <td><code>class</code></td>
      <td>Once per test class</td>
    </tr>
    <tr>
      <td><code>module</code></td>
      <td>Once per module (i.e., per Python file)</td>
    </tr>
    <tr>
      <td><code>session</code></td>
      <td>Once for the entire test session (across all files)</td>
    </tr>
  </tbody>
</table>
<h3>Example Structure</h3>
<p>We’ll use print statements to visualize when the fixtures are being set up and torn down.</p>
<p>1. Add these new fixtures into <strong><code>conftest.py</code></strong></p>
<pre><code># conftest.py 

@pytest.fixture(scope="class")
def class_resource():
    print("\n[SETUP] class_resource")
    yield "class fixture"
    print("[TEARDOWN] class_resource")

@pytest.fixture(scope="module")
def module_resource():
    print("\n[SETUP] module_resource")
    yield "module fixture"
    print("[TEARDOWN] module_resource")

@pytest.fixture(scope="session")
def session_resource():
    print("\n[SETUP] session_resource")
    yield "session fixture"
    print("[TEARDOWN] session_resource")</code></pre>
    <p>2. Create a new script: <strong><code>tests/test_scopes_1.py</code></strong></p>
    <pre><code># test_scopes.py
import pytest

class TestGroupA:
    def test_one(self, class_resource, module_resource, session_resource):
        print("Running TestGroupA.test_one")

    def test_two(self, class_resource, module_resource, session_resource):
        print("Running TestGroupA.test_two")


class TestGroupB:
    def test_three(self, class_resource, module_resource, session_resource):
        print("Running TestGroupB.test_three")
</code></pre>
    <p>3. Expected Output When Running:</p>
    <pre><code>pytest tests/test_scopes_1.py -s</code></pre>
    <pre><code># OUTPUT terminal

[SETUP] session_resource
[SETUP] module_resource

[SETUP] class_resource
Running TestGroupA.test_one
Running TestGroupA.test_two
[TEARDOWN] class_resource

[SETUP] class_resource
Running TestGroupB.test_three
[TEARDOWN] class_resource

[TEARDOWN] module_resource
[TEARDOWN] session_resource
</code></pre>
<h3>Analysis</h3>

<ol>
  <li>
    <strong>Session Scope (<code>scope="session"</code>)</strong>
    <ul>
      <li>Initialized once for the whole test run.</li>
      <li>Shared across all modules and test classes.</li>
      <li>Use case: expensive setup like DB connection, Docker container.</li>
    </ul>
  </li>

  <li>
    <strong>Module Scope (<code>scope="module"</code>)</strong>
    <ul>
      <li>Runs once per Python test file.</li>
      <li>Even if you have multiple classes in one file, it will still run only once.</li>
      <li>Use case: file-level shared resource like reading static data.</li>
    </ul>
  </li>

  <li>
    <strong>Class Scope (<code>scope="class"</code>)</strong>
    <ul>
      <li>Runs once per test class.</li>
      <li>All methods in the same test class share the same fixture instance.</li>
      <li>Use case: preparing resources specific to test class behavior.</li>
    </ul>
  </li>
</ol>


    </div>

   


    <div class="slide">
        <h2>5. Parallel Execution with `pytest-xdist`</h2>
        <p><strong>pytest-xdist</strong> is a Pytest plugin that allows you to:</p>
        <ul>
        <li><strong>Run tests in parallel</strong> (multiple test processes at the same time).</li>
        <li><strong>Distribute tests</strong> across multiple CPUs or even different machines.</li>
        <li><strong>Speed up test suite execution</strong>, especially when there are many long-running tests.</li>
        </ul>
        <h3>Installation:</h3>
        <pre><code class="language-python">pip install pytest-xdist</code></pre>
        <h3>Usage:</h3>
        <pre><code class="language-python"># Run tests in 2 parallel processes locally
pytest -n 2</code></pre>
        <h3>Structure project example: (<code>teste-tools</code>)</h3>
          <div class="tree">tests/
├── test_demoqa.py
├── test_buttons.py
├── test_math.py
└── test_strings.py</div>
    <h3>Running in parallel</code>:</h3>
    <pre><code>% pytest -n 2
===================================================== test session starts =====================================================
platform darwin -- Python 3.9.6, pytest-8.4.2, pluggy-1.6.0
rootdir: /Users/rodrigoprado/rgp/ProgramaEstagio/teste-tools
plugins: xdist-3.8.0
2 workers [9 items]     
.........</code></pre>
    </div>

    <div class="slide">
        <h3>Happy automation testing!</h3>
    </div>

</body>
</html>